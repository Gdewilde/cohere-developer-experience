post:
  x-internal: true
  summary: LogLikelihood
  operationId: loglikelihood
  parameters:
    - $ref: './types/RequestSource.yaml'
  responses:
    '400':
      $ref: "./errors/BadRequest.yaml"
    '401':
      $ref: "./errors/Unauthorized.yaml"
    '403':
      $ref: "./errors/Forbidden.yaml"
    '404':
      $ref: "./errors/NotFound.yaml"
    '422':
      $ref: "./errors/UnprocessableEntity.yaml"
    '429':
      $ref: './errors/RateLimit.yaml'
    '498':
      $ref: "./errors/InvalidToken.yaml"
    '499':
      $ref: "./errors/RequestCancelled.yaml"
    '500':
      $ref: "./errors/InternalServerError.yaml"
    '501':
      $ref: "./errors/NotImplemented.yaml"
    '503':
      $ref: "./errors/ServiceUnavailable.yaml"
    '504':
      $ref: "./errors/GatewayTimeout.yaml"
    '200':
      description: OK
      content:
        application/json:
          schema:
            $ref: './types/LogLikelihoodResponse.yaml'
      headers:
        X-API-Warning:
          schema:
            type: string
            x-fern-audiences: ['public']
          description: Warning description for incorrect usage of the API
  description: Log likelihood tokenizes the input and annotates the tokens with
    the models assessment of their probability.
  requestBody:
    content:
      application/json:
        schema:
          type: object
          x-fern-audiences: ['public']
          x-examples:
            Prompt and completion:
              value:
                prompt: Please explain to me how LLMs work
                completion: They work by using a large language model to generate text.
            Raw prompt:
              value:
                raw_prompt: <BOS_TOKEN>What's the colour of the sky?<EOP_TOKEN> blue<EOS_TOKEN>
          properties:
            model:
              type: string
              x-fern-audiences: ['public']
              description: The identifier of the model to generate with. Currently available
                models are `command` (default), `command-nightly`
                (experimental), `command-light`, and `command-light-nightly`
                (experimental). Smaller, "light" models are faster, while larger
                models will perform better. [Custom
                models](/docs/training-custom-models) can also be supplied with
                their full ID.
            prompt:
              type: string
              x-fern-audiences: ['public']
              description: |
                To be used in conjunction with `completion`. This will be interpolated into the user message in the prompt template.
              writeOnly: true
            completion:
              type: string
              x-fern-audiences: ['public']
              description: |
                To be used in conjunction with `prompt`. This will be interpolated into the chatbot reponse in the prompt template.
            raw_prompt:
              type: string
              x-fern-audiences: ['public']
              description: |
                To be used on its own, this allows you to pass a custom prompt to the model that will not be interpolated into a prompt template.
              writeOnly: true
