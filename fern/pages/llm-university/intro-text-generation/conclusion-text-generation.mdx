---
title: "Conclusion - Text Generation"
slug: "docs/conclusion-text-generation"

hidden: false
createdAt: "Thu Mar 21 2024 18:16:23 GMT+0000 (Coordinated Universal Time)"
updatedAt: "Thu Mar 28 2024 16:38:32 GMT+0000 (Coordinated Universal Time)"
---
Congratulations! In this module, you've learned how to build a chatbot with Cohere’s Chat endpoint, and you’ve developed a conceptual understanding of how it works. You also learned how to design prompts, adjust parameters, and fine-tune on custom data to get the output that you want.

## What’s Next?

LLM University (LLMU) has many modules that you can use to take your chatbots to the next level.

### Prompt Engineering Module

As you learned in the [Prompt Engineering Basics chapter](/docs/prompt-engineering-basics) of this module, when working with LLM chatbots, the prompt is the key to getting the desired response. A well-designed prompt will result in useful and accurate responses from a model, and it will considerably improve your experience interacting with it. In the [Prompt Engineering module](/docs/intro-prompt-engineering), you’ll go deeper into prompt engineering techniques and apply them to Cohere’s Command model.

### Retrieval-Augmented Generation (RAG) Module

In the [Introduction to RAG chapter](/docs/introduction-to-rag) of this module, you learned basic concepts about RAG, a method for allowing a chatbot to access external data sources – like the internet or a company’s internal technical documentation – which leads to better, more factual generations. In the [Retrieval-Augmented Generation module](/docs/module-8-chat-and-retrieval-augmented-generation-rag), you will learn how to use the Chat endpoint’s RAG feature to build your own RAG-powered chatbots.
